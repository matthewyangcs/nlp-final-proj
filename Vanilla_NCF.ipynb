{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8d309f-6b73-4697-95f4-eb449c2ede62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d4bcbd-9676-4775-aa2f-990f305c3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter\n",
    "import multiprocessing\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.formula.api as smf\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd0955-a340-4b06-9c5b-d94ad324e4d1",
   "metadata": {},
   "source": [
    "# Loading Processed Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62734bd0-0272-4a10-9622-7a34a46a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROCESSED_FOLDER = './data/processed/'\n",
    "# PROCESSED_REVIEWS_FILE = 'processed_reviews.csv'\n",
    "PROCESSED_REVIEWS_FILE = 'processed_reviews_with_sentiment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41db13ef-f58f-4be8-b789-5f5f94e7c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(os.path.join(PROCESSED_FOLDER, PROCESSED_REVIEWS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f769cfe-cd8f-4a96-bc0c-569c5dfabd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>neg_sent_avg</th>\n",
       "      <th>neu_sent_avg</th>\n",
       "      <th>pos_sent_avg</th>\n",
       "      <th>compound_sent_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>First things first. My \"reviews\" system is exp...</td>\n",
       "      <td>8</td>\n",
       "      <td>[['First', 'things', 'first', '.'], ['My', '``...</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.757708</td>\n",
       "      <td>0.168937</td>\n",
       "      <td>0.208675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259117</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Let me start off by saying that Made in Abyss ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['Let', 'me', 'start', 'off', 'by', 'saying',...</td>\n",
       "      <td>0.062741</td>\n",
       "      <td>0.759333</td>\n",
       "      <td>0.177963</td>\n",
       "      <td>0.302804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253664</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Art 9/10: It is great, especially the actions ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[['Art', '9/10', ':', 'It', 'is', 'great', ','...</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>0.792889</td>\n",
       "      <td>0.159833</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247454</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>As someone who loves Studio Ghibli and its mov...</td>\n",
       "      <td>6</td>\n",
       "      <td>[['As', 'someone', 'who', 'loves', 'Studio', '...</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.187896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23791</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>code geass is one of those series that everybo...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['code', 'geass', 'is', 'one', 'of', 'those',...</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.248143</td>\n",
       "      <td>0.534129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  user_id  item_id  \\\n",
       "0     255938        0        1   \n",
       "1     259117        1        2   \n",
       "2     253664        2        3   \n",
       "3     247454        3        4   \n",
       "4      23791        4        5   \n",
       "\n",
       "                                                text  rating  \\\n",
       "0  First things first. My \"reviews\" system is exp...       8   \n",
       "1  Let me start off by saying that Made in Abyss ...      10   \n",
       "2  Art 9/10: It is great, especially the actions ...       7   \n",
       "3  As someone who loves Studio Ghibli and its mov...       6   \n",
       "4  code geass is one of those series that everybo...      10   \n",
       "\n",
       "                                      tokenized_text  neg_sent_avg  \\\n",
       "0  [['First', 'things', 'first', '.'], ['My', '``...      0.073333   \n",
       "1  [['Let', 'me', 'start', 'off', 'by', 'saying',...      0.062741   \n",
       "2  [['Art', '9/10', ':', 'It', 'is', 'great', ','...      0.047278   \n",
       "3  [['As', 'someone', 'who', 'loves', 'Studio', '...      0.055577   \n",
       "4  [['code', 'geass', 'is', 'one', 'of', 'those',...      0.028857   \n",
       "\n",
       "   neu_sent_avg  pos_sent_avg  compound_sent_avg  \n",
       "0      0.757708      0.168937           0.208675  \n",
       "1      0.759333      0.177963           0.302804  \n",
       "2      0.792889      0.159833           0.220767  \n",
       "3      0.809192      0.135269           0.187896  \n",
       "4      0.723000      0.248143           0.534129  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8dc21-b48d-47c4-92b9-5ecade9e196b",
   "metadata": {},
   "source": [
    "# Converting Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13a2335-8f0b-4ad8-9590-912dc3871c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert item_id to 0 indexed\n",
    "if min(reviews['item_id']) != 0:\n",
    "    reviews['item_id'] = reviews['item_id'] - 1\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2066316-854c-4484-88cc-8043d5eb0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Review:\n",
    "    user_id: int\n",
    "    item_id: int\n",
    "    rating: int\n",
    "    text: str\n",
    "    pos_sent: float\n",
    "    neg_sent: float\n",
    "    compound_sent: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7786b8c-8d87-4e40-aea0-1d5a7933e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_KEY = 'user_id'\n",
    "ITEM_KEY = 'item_id'\n",
    "RATING_KEY = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67804fba-a838-4d53-ab38-dfc529312286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>neg_sent_avg</th>\n",
       "      <th>neu_sent_avg</th>\n",
       "      <th>pos_sent_avg</th>\n",
       "      <th>compound_sent_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>First things first. My \"reviews\" system is exp...</td>\n",
       "      <td>8</td>\n",
       "      <td>[['First', 'things', 'first', '.'], ['My', '``...</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.757708</td>\n",
       "      <td>0.168937</td>\n",
       "      <td>0.208675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259117</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Let me start off by saying that Made in Abyss ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['Let', 'me', 'start', 'off', 'by', 'saying',...</td>\n",
       "      <td>0.062741</td>\n",
       "      <td>0.759333</td>\n",
       "      <td>0.177963</td>\n",
       "      <td>0.302804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253664</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Art 9/10: It is great, especially the actions ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[['Art', '9/10', ':', 'It', 'is', 'great', ','...</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>0.792889</td>\n",
       "      <td>0.159833</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247454</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>As someone who loves Studio Ghibli and its mov...</td>\n",
       "      <td>6</td>\n",
       "      <td>[['As', 'someone', 'who', 'loves', 'Studio', '...</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.187896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23791</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>code geass is one of those series that everybo...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['code', 'geass', 'is', 'one', 'of', 'those',...</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.248143</td>\n",
       "      <td>0.534129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  user_id  item_id  \\\n",
       "0     255938        0        0   \n",
       "1     259117        1        1   \n",
       "2     253664        2        2   \n",
       "3     247454        3        3   \n",
       "4      23791        4        4   \n",
       "\n",
       "                                                text  rating  \\\n",
       "0  First things first. My \"reviews\" system is exp...       8   \n",
       "1  Let me start off by saying that Made in Abyss ...      10   \n",
       "2  Art 9/10: It is great, especially the actions ...       7   \n",
       "3  As someone who loves Studio Ghibli and its mov...       6   \n",
       "4  code geass is one of those series that everybo...      10   \n",
       "\n",
       "                                      tokenized_text  neg_sent_avg  \\\n",
       "0  [['First', 'things', 'first', '.'], ['My', '``...      0.073333   \n",
       "1  [['Let', 'me', 'start', 'off', 'by', 'saying',...      0.062741   \n",
       "2  [['Art', '9/10', ':', 'It', 'is', 'great', ','...      0.047278   \n",
       "3  [['As', 'someone', 'who', 'loves', 'Studio', '...      0.055577   \n",
       "4  [['code', 'geass', 'is', 'one', 'of', 'those',...      0.028857   \n",
       "\n",
       "   neu_sent_avg  pos_sent_avg  compound_sent_avg  \n",
       "0      0.757708      0.168937           0.208675  \n",
       "1      0.759333      0.177963           0.302804  \n",
       "2      0.792889      0.159833           0.220767  \n",
       "3      0.809192      0.135269           0.187896  \n",
       "4      0.723000      0.248143           0.534129  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cf6817-3799-4833-85b7-cf46631c16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_reviews = defaultdict(list)\n",
    "for _, row in reviews.iterrows():\n",
    "    user_id, item_id, rating, text = row[USER_KEY], row[ITEM_KEY], row[RATING_KEY], row['text']\n",
    "    pos_sent, neg_sent, compound_sent = row['pos_sent_avg'], row['neg_sent_avg'], row['compound_sent_avg']\n",
    "    user_to_reviews[user_id].append(Review(user_id, item_id, rating, text, pos_sent, neg_sent, compound_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aac691-4002-4fbb-89fb-905f46d5c993",
   "metadata": {},
   "source": [
    "## Creating the score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2741bc-2256-4222-85b3-12148559e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users by items\n",
    "X = np.zeros(shape=(reviews['user_id'].nunique(), reviews['item_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b66889-3914-4661-87c9-4d6bcb04b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in reviews.iterrows():\n",
    "    user_id, item_id, rating = row[USER_KEY], row[ITEM_KEY], row[RATING_KEY]\n",
    "    X[user_id][item_id] = rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4cbb9-e530-4f74-a040-8e25ff0400be",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f8871e-2a53-4338-a7db-42c251f91baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = copy.deepcopy(X)\n",
    "valid_X = np.zeros(shape=X.shape)\n",
    "test_X = np.zeros(shape=X.shape)\n",
    "\n",
    "for user_id, reviews in user_to_reviews.items():\n",
    "    # can confirm this actually shuffles properly (this code block works)\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    # Leave one out for valid\n",
    "    valid_review = reviews[0]\n",
    "    train_X[valid_review.user_id][valid_review.item_id] = 0\n",
    "    valid_X[valid_review.user_id][valid_review.item_id] = valid_review.rating\n",
    "    \n",
    "    # Leave one out for test\n",
    "    test_review = reviews[1]\n",
    "    train_X[test_review.user_id][test_review.item_id] = 0\n",
    "    test_X[test_review.user_id][test_review.item_id] = test_review.rating\n",
    "    \n",
    "    # Rest for train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc5daa-0f72-4c5e-968a-b1adea2a7a3d",
   "metadata": {},
   "source": [
    "## Creating bias terms for users / items from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e4f205-bd38-41b7-8bc0-d315ff486b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users\n",
    "user_to_pos_sent = defaultdict(list)\n",
    "user_to_neg_sent = defaultdict(list)\n",
    "user_to_compound_sent = defaultdict(list)\n",
    "\n",
    "# items\n",
    "item_to_pos_sent = defaultdict(list)\n",
    "item_to_neg_sent = defaultdict(list)\n",
    "item_to_compound_sent = defaultdict(list)\n",
    "\n",
    "# loadding\n",
    "for user_id, reviews in user_to_reviews.items():\n",
    "    for r in reviews:\n",
    "        # skip if not in train\n",
    "        if train_X[user_id, r.item_id] == 0:\n",
    "            continue\n",
    "        user_to_pos_sent[user_id].append(r.pos_sent)\n",
    "        user_to_neg_sent[user_id].append(r.neg_sent)\n",
    "        user_to_compound_sent[user_id].append(r.compound_sent)\n",
    "        item_to_pos_sent[r.item_id].append(r.pos_sent)\n",
    "        item_to_neg_sent[r.item_id].append(r.neg_sent)\n",
    "        item_to_compound_sent[r.item_id].append(r.compound_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d030b3d-53be-433c-a5ff-3e0c774d3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging values to get bias term\n",
    "def list_mapping_to_float_mapping(hm: dict):\n",
    "    id_to_sent_term = defaultdict(float)\n",
    "    for k, v in hm.items():\n",
    "        id_to_sent_term[k] = np.mean(v)\n",
    "    return id_to_sent_term\n",
    "\n",
    "user_to_pos_sent_term = list_mapping_to_float_mapping(user_to_pos_sent)\n",
    "user_to_neg_sent_term = list_mapping_to_float_mapping(user_to_neg_sent)\n",
    "user_to_compound_sent_term = list_mapping_to_float_mapping(user_to_compound_sent)\n",
    "item_to_pos_sent_term = list_mapping_to_float_mapping(item_to_pos_sent)\n",
    "item_to_neg_sent_term = list_mapping_to_float_mapping(item_to_neg_sent)\n",
    "item_to_compound_sent_term = list_mapping_to_float_mapping(item_to_compound_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d1ea0-2053-4428-b586-df8fca47c966",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GMF, MLP, and NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b075113-a9f4-4ee3-9d09-743659ddc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(values):\n",
    "    return torch.sum(torch.square(values))\n",
    "\n",
    " \n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=20, regularization_constant=1e-6, sentiment_regularization_constant=1e-6, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_factors = nn.Embedding(num_items, embedding_dim)\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.sentiment_regularization_constant = sentiment_regularization_constant # unused\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor):\n",
    "        # (users, emb_dim) * (items, emb_dim) = (interactions, emb_dim)\n",
    "        result_tensor = self.user_factors(users) * self.item_factors(items)\n",
    "        user_latent_factors = self.user_factors(users)\n",
    "        item_latent_factors = self.item_factors(items)\n",
    "        pred_ratings = user_latent_factors @ item_latent_factors.T\n",
    "        pred_ratings = 1 + 9 * torch.sigmoid(pred_ratings)\n",
    "        return pred_ratings.diagonal()\n",
    "    \n",
    "    def loss(self, pred_rating: torch.LongTensor, rating: torch.LongTensor, rmse=False):\n",
    "        if rmse:\n",
    "            loss = torch.sqrt(F.mse_loss(pred_rating, rating) + self.eps)\n",
    "        else:\n",
    "            loss = F.mse_loss(pred_rating, rating) + self.eps\n",
    "        \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_values = l2_regularization(self.user_factors.weight) + l2_regularization(self.item_factors.weight)\n",
    "        l2_penalty = (1/len(rating)) * self.regularization_constant * sum_of_squared_values\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = loss + l2_penalty\n",
    "        return total_loss\n",
    "    \n",
    "    def predict_single_interaction(self, user_id: int, item_id: int):\n",
    "        user = torch.LongTensor([user_id]).cuda()\n",
    "        item = torch.LongTensor([item_id]).cuda()\n",
    "        return self.forward(user, item)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=20, regularization_constant=1e-6, sentiment_regularization_constant=1e-6, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_factors = nn.Embedding(num_items, embedding_dim)\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.sentiment_regularization_constant = sentiment_regularization_constant # unused\n",
    "        self.eps = eps\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(2*embedding_dim, 128)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor):\n",
    "        # user is shape (users, 1)\n",
    "        # item is shape (items, 1)\n",
    "        # embedding output shape is (*, emb_dim) = (users/items, emb_dim)\n",
    "        user_latent_factors = self.user_factors(users)\n",
    "        item_latent_factors = self.item_factors(items)\n",
    "        \n",
    "        # Concat latent facts together => (*, 2*emb_dim)\n",
    "        user_item_latent_factors = torch.cat((user_latent_factors, item_latent_factors), dim=1)\n",
    "\n",
    "        # FC takes (*, in_dim) and outputs (*, out_dim)\n",
    "        output = self.fc1(user_item_latent_factors)\n",
    "        output = self.relu1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.fc3(output)\n",
    "        \n",
    "        # Clip in the desired range\n",
    "        pred_ratings = 1 + 9 * torch.sigmoid(output).squeeze()  \n",
    "\n",
    "        return pred_ratings\n",
    "    \n",
    "    def loss(self, pred_rating: torch.LongTensor, rating: torch.LongTensor, rmse=False):\n",
    "        if rmse:\n",
    "            loss = torch.sqrt(F.mse_loss(pred_rating, rating) + self.eps)\n",
    "        else:\n",
    "            loss = F.mse_loss(pred_rating, rating) + self.eps\n",
    "        \n",
    "        \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_values = l2_regularization(self.user_factors.weight) + l2_regularization(self.item_factors.weight)\n",
    "        l2_penalty = (1/len(rating)) * self.regularization_constant * sum_of_squared_values\n",
    "        \n",
    "        # Total Loss\n",
    "        total_loss = loss + l2_penalty\n",
    "        return total_loss\n",
    "    \n",
    "    def predict_single_interaction(self, user_id: int, item_id: int):\n",
    "        user = torch.LongTensor([user_id]).cuda()\n",
    "        item = torch.LongTensor([item_id]).cuda()\n",
    "        return self.forward(user, item)\n",
    "\n",
    "    \n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, MF_embedding_dim=20, MLP_embedding_dim=20, alpha=0.5, regularization_constant=1e-6, sentiment_regularization_constant=1e-6, eps=1e-8):\n",
    "        super().__init__()\n",
    "        # User factors\n",
    "        self.MF_user_factors = nn.Embedding(num_users, MF_embedding_dim)\n",
    "        self.MLP_user_factors = nn.Embedding(num_users, MLP_embedding_dim)\n",
    "        \n",
    "        # Item Factors\n",
    "        self.MF_item_factors = nn.Embedding(num_users, MF_embedding_dim)\n",
    "        self.MLP_item_factors = nn.Embedding(num_users, MLP_embedding_dim)\n",
    "\n",
    "        \n",
    "        # GMF Layer\n",
    "        # element wise product\n",
    "        \n",
    "        # MLP Layers\n",
    "        self.fc1 = nn.Linear(2*MLP_embedding_dim, 128)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "        # Trainable NeuMF Layer (in the original paper it's just a hyperparameter)\n",
    "        # Initialize to [0.5, 0.5]\n",
    "        self.neumf_layer = nn.Linear(2, 1)\n",
    "        self.neumf_layer.weight.data = torch.Tensor([[0.5, 0.5]]).to(device)\n",
    "        \n",
    "        # alpha * MF + (1-alpha) * mlp\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        ## The usual constants\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.sentiment_regularization_constant = sentiment_regularization_constant # unused\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor):\n",
    "        # There are all (users x emb_dim), as usual\n",
    "        MF_latent_user_factors = self.MF_user_factors(users)\n",
    "        MLP_latent_user_factors = self.MLP_user_factors(users)\n",
    "        \n",
    "        MLP_latent_item_factors = self.MLP_item_factors(items)\n",
    "        MF_latent_item_factors = self.MF_item_factors(items)\n",
    "        \n",
    "        # GMF Layer\n",
    "#         gmf_output = (MF_latent_user_factors @ MF_latent_item_factors.T).diagonal().unsqueeze(1)\n",
    "        gmf_output = 1 + 9 * torch.sigmoid((MF_latent_user_factors @ MF_latent_item_factors.T).diagonal())\n",
    "        \n",
    "        # MLP Layers\n",
    "        concat_latent_factors = torch.cat((MLP_latent_user_factors, MLP_latent_item_factors), dim=1)\n",
    "        mlp_output = self.fc1(concat_latent_factors)\n",
    "        mlp_output = self.relu1(mlp_output)\n",
    "        mlp_output = self.fc2(mlp_output)\n",
    "        mlp_output = self.relu2(mlp_output)\n",
    "        mlp_output = self.fc3(mlp_output)\n",
    "        mlp_output = (1 + 9 * torch.sigmoid(mlp_output)).squeeze()\n",
    "        \n",
    "#         print(gmf_output, mlp_output)\n",
    "#         print(gmf_output.shape, mlp_output.shape)\n",
    "        # NeuMF Layer\n",
    "#         combined_output = torch.cat((gmf_output, mlp_output), dim=1)\n",
    "#         pred_ratings = self.neumf_layer(combined_output).squeeze()\n",
    "\n",
    "        return torch.add(self.alpha * gmf_output, (1 - self.alpha) * mlp_output) \n",
    "    \n",
    "\n",
    "    \n",
    "    def loss(self, pred_rating: torch.LongTensor, rating: torch.LongTensor, rmse=False):\n",
    "        if rmse:\n",
    "            loss = torch.sqrt(F.mse_loss(pred_rating, rating) + self.eps)\n",
    "        else:\n",
    "            loss = F.mse_loss(pred_rating, rating) + self.eps\n",
    "        \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_values = l2_regularization(self.MF_user_factors.weight) + l2_regularization(self.MF_item_factors.weight)\n",
    "        sum_of_squared_values += l2_regularization(self.MLP_user_factors.weight)\n",
    "        sum_of_squared_values += l2_regularization(self.MLP_item_factors.weight)\n",
    "        for f in [self.fc1, self.fc2, self.fc3]:\n",
    "            sum_of_squared_values += l2_regularization(f.weight)\n",
    "        l2_penalty = (1/len(rating)) * self.regularization_constant * sum_of_squared_values\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = loss + l2_penalty\n",
    "        return total_loss\n",
    "    \n",
    "    def predict_single_interaction(self, user_id: int, item_id: int):\n",
    "        user = torch.LongTensor([user_id, user_id+1]).cuda()\n",
    "        item = torch.LongTensor([item_id, item_id+1]).cuda()\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ccfc7c5-88ec-47ad-aced-9f57b6e60bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_MSE_loss(eval_X, model, round_digits=3):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)]).to(device)\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list).to(device)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list).to(device)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(F.mse_loss(pred_ratings, gt_ratings).item(), 3)\n",
    "\n",
    "def eval_RMSE_loss(eval_X, model):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)]).to(device)\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list).to(device)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list).to(device)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(torch.sqrt(F.mse_loss(pred_ratings, gt_ratings)).item(), 3)\n",
    "\n",
    "def eval_MAE_loss(eval_X, model):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)]).to(device)\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list).to(device)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list).to(device)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(F.l1_loss(pred_ratings, gt_ratings).item(), 3)\n",
    "\n",
    "\n",
    "def train_v2(train_X, valid_X, model, optimizer, n_epochs=10, batch_size=5, rmse=False):\n",
    "    \"\"\"Training Function, calculates training and validation loss\"\"\"\n",
    "    \n",
    "    for epoch in (range(1, n_epochs+1)):\n",
    "        users, items = train_X.nonzero()\n",
    "        num_examples = len(users)\n",
    "        permuted_indices = np.random.permutation(num_examples)\n",
    "        users, items = users[permuted_indices], items[permuted_indices]\n",
    "        \n",
    "\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(num_examples // batch_size)):\n",
    "            user_ids_list = users[i*batch_size:i*batch_size+batch_size]\n",
    "            item_ids_list = items[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Turn data into tensors\n",
    "            ratings = torch.FloatTensor([train_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)]).to(device)\n",
    "            curr_users_tensor = torch.LongTensor(user_ids_list).to(device)\n",
    "            curr_items_tensor = torch.LongTensor(item_ids_list).to(device)\n",
    "\n",
    "            # Predict and calculate loss\n",
    "            pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "            assert pred_ratings.shape == ratings.shape\n",
    "            \n",
    "            ## SELECTING LOSS HERE\n",
    "            # loss = model.loss(pred_rating, rating)\n",
    "            loss = model.loss(pred_ratings, ratings, rmse=rmse)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # MSE Loss w/o regularization (just for status updates)\n",
    "            total_train_loss += F.mse_loss(pred_ratings, ratings, reduction='sum')\n",
    "\n",
    "        # Computing validation loss for display\n",
    "        total_valid_loss = eval_MSE_loss(valid_X, model)\n",
    "        total_valid_RMSE_loss = eval_RMSE_loss(valid_X, model)\n",
    "        total_valid_MAE_loss = eval_MAE_loss(valid_X, model)\n",
    "        \n",
    "        print(f\"Epoch {epoch} MSE Loss: {round(total_train_loss.item() / (batch_size * (num_examples//batch_size)), 3)}, valid MSE Loss: {total_valid_loss}, valid RMSE Loss: {total_valid_RMSE_loss}, valid MAE Loss: {total_valid_MAE_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68147f34-7dda-4d23-8269-ad1997f26640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(test_X, model):\n",
    "    total_test_loss = eval_MSE_loss(test_X, model)\n",
    "    total_test_RMSE_loss = eval_RMSE_loss(test_X, model)\n",
    "    total_test_MAE_loss = eval_MAE_loss(test_X, model)\n",
    "    print(f\"test MSE Loss: {total_test_loss}, test RMSE Loss: {total_test_RMSE_loss}, test MAE Loss: {total_test_MAE_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f53fe5-9de6-4808-832b-c4bc365c9686",
   "metadata": {},
   "source": [
    "## Config Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba0265-0529-4485-9e20-29faf8bb9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d5a0326-0258-4111-84b7-a978f80e8bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], device='cuda:7')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Forcing GPU\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(\"cuda:7\")\n",
    "device = torch.device(\"cuda:7\")\n",
    "a = torch.tensor([[1., 2.], [3., 4.]]).to(device)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e0d79-d054-4bef-889e-7f88187c0ba2",
   "metadata": {},
   "source": [
    "## Training GMF and MLP Independently\n",
    "As per the NCF paper, we must train GMF and MLP separately. Then, we initialize NeuMF using the pretrained models of GMF and MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12498052-3871-4d4b-9b35-3ad7ac943a15",
   "metadata": {},
   "source": [
    "### Training GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96bac942-b9c7-43ea-ac27-ba22eadac32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0000], device='cuda:7', grad_fn=<DiagonalBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "embedding_dim=200\n",
    "lr=.01\n",
    "regularization_constant=.01\n",
    "\n",
    "# Test\n",
    "gmf = GMF(num_users=X.shape[0], num_items=X.shape[1], embedding_dim=embedding_dim, regularization_constant=regularization_constant)\n",
    "optimizer = torch.optim.Adam(gmf.parameters(), lr=lr)\n",
    "gmf.to(device)\n",
    "gmf.predict_single_interaction(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1c014b0-1ef5-4d91-beda-38eb31d34c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 345.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 MSE Loss: 15.794, valid MSE Loss: 9.327, valid RMSE Loss: 3.054, valid MAE Loss: 2.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 304.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 MSE Loss: 4.093, valid MSE Loss: 5.261, valid RMSE Loss: 2.294, valid MAE Loss: 1.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 380.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 MSE Loss: 2.334, valid MSE Loss: 4.433, valid RMSE Loss: 2.105, valid MAE Loss: 1.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 398.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 MSE Loss: 2.199, valid MSE Loss: 4.179, valid RMSE Loss: 2.044, valid MAE Loss: 1.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 322.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 MSE Loss: 2.317, valid MSE Loss: 4.271, valid RMSE Loss: 2.067, valid MAE Loss: 1.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 347.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 MSE Loss: 2.469, valid MSE Loss: 4.28, valid RMSE Loss: 2.069, valid MAE Loss: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 349.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 MSE Loss: 2.549, valid MSE Loss: 4.165, valid RMSE Loss: 2.041, valid MAE Loss: 1.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 364.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 MSE Loss: 2.524, valid MSE Loss: 4.254, valid RMSE Loss: 2.062, valid MAE Loss: 1.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 349.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 MSE Loss: 2.473, valid MSE Loss: 4.082, valid RMSE Loss: 2.02, valid MAE Loss: 1.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 333.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 MSE Loss: 2.368, valid MSE Loss: 4.012, valid RMSE Loss: 2.003, valid MAE Loss: 1.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 304.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 MSE Loss: 2.27, valid MSE Loss: 3.993, valid RMSE Loss: 1.998, valid MAE Loss: 1.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 408.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 MSE Loss: 2.198, valid MSE Loss: 3.87, valid RMSE Loss: 1.967, valid MAE Loss: 1.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 345.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 MSE Loss: 2.082, valid MSE Loss: 3.992, valid RMSE Loss: 1.998, valid MAE Loss: 1.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 344.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 MSE Loss: 2.012, valid MSE Loss: 3.731, valid RMSE Loss: 1.932, valid MAE Loss: 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 352.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 MSE Loss: 1.895, valid MSE Loss: 3.778, valid RMSE Loss: 1.944, valid MAE Loss: 1.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 340.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 MSE Loss: 1.842, valid MSE Loss: 3.707, valid RMSE Loss: 1.925, valid MAE Loss: 1.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 450.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 MSE Loss: 1.804, valid MSE Loss: 3.806, valid RMSE Loss: 1.951, valid MAE Loss: 1.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 449.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 MSE Loss: 1.744, valid MSE Loss: 3.822, valid RMSE Loss: 1.955, valid MAE Loss: 1.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 422.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 MSE Loss: 1.702, valid MSE Loss: 3.769, valid RMSE Loss: 1.942, valid MAE Loss: 1.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 362.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 MSE Loss: 1.662, valid MSE Loss: 3.743, valid RMSE Loss: 1.935, valid MAE Loss: 1.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 355.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 MSE Loss: 1.633, valid MSE Loss: 3.729, valid RMSE Loss: 1.931, valid MAE Loss: 1.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 344.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 MSE Loss: 1.611, valid MSE Loss: 3.799, valid RMSE Loss: 1.949, valid MAE Loss: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 313.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 MSE Loss: 1.599, valid MSE Loss: 3.772, valid RMSE Loss: 1.942, valid MAE Loss: 1.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 300.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 MSE Loss: 1.586, valid MSE Loss: 3.696, valid RMSE Loss: 1.923, valid MAE Loss: 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:01<00:00, 346.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 MSE Loss: 1.568, valid MSE Loss: 3.744, valid RMSE Loss: 1.935, valid MAE Loss: 1.543\n"
     ]
    }
   ],
   "source": [
    "train_v2(train_X, valid_X, gmf, optimizer, n_epochs=25, batch_size=64, rmse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c4773fe-da72-45e3-8098-4506195b0337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE Loss: 3.688, test RMSE Loss: 1.921, test MAE Loss: 1.547\n"
     ]
    }
   ],
   "source": [
    "eval_model(test_X, gmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706de2c8-30ee-4766-ad24-636feb6adbe2",
   "metadata": {},
   "source": [
    "## Training MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07294869-0c4d-4e51-9b43-a77fc80497b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0489, device='cuda:7', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=200\n",
    "lr=1e-2\n",
    "regularization_constant=.011\n",
    "\n",
    "mlp = MLP(num_users=X.shape[0], num_items=X.shape[1], \n",
    "                    embedding_dim=embedding_dim, \n",
    "                    regularization_constant=regularization_constant,\n",
    "                )\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "mlp.to(device)\n",
    "mlp.predict_single_interaction(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317ab48-a4fe-4cfa-b31e-dcfb2039aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "train_v2(train_X, valid_X, mlp, optimizer, n_epochs=5, batch_size=64, rmse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00abe88c-6219-4add-b889-ded0d444f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE Loss: 2.906, test RMSE Loss: 1.705, test MAE Loss: 1.302\n"
     ]
    }
   ],
   "source": [
    "eval_model(test_X, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e0e20-2eb2-47cb-b523-8fd9e12bc9ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training NeuMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c96ba-8163-47a6-8b80-445c5e2cf90d",
   "metadata": {},
   "source": [
    "### Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48722b36-3e75-4f86-bbc0-fb7cb6a3c087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.0125, 3.4545], device='cuda:7', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Decent set of hyper params\n",
    "# MF_embedding_dim=200\n",
    "# MLP_embedding_dim=200\n",
    "# lr=1e-3\n",
    "# regularization_constant=1e-2\n",
    "# alpha = 0.5\n",
    "\n",
    "# neumf = NeuMF(num_users=X.shape[0], num_items=X.shape[1], \n",
    "#                     MF_embedding_dim=MF_embedding_dim, \n",
    "#                     MLP_embedding_dim=MLP_embedding_dim,\n",
    "#                     alpha=alpha,\n",
    "#                     regularization_constant=regularization_constant,\n",
    "#                 )\n",
    "\n",
    "# optimizer = torch.optim.SGD(neumf.parameters(), lr=lr)\n",
    "# neumf.to(device)\n",
    "# neumf.predict_single_interaction(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efd1a4e9-d1a6-42ee-9492-1ae31dd32b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.9633, 6.4596], device='cuda:7', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Experimentation Params\n",
    "MF_embedding_dim=200 # Must match gmf\n",
    "MLP_embedding_dim=200 # Must match mlp\n",
    "lr=1e-3\n",
    "regularization_constant=1e-2\n",
    "alpha = 0.3\n",
    "\n",
    "neumf = NeuMF(num_users=X.shape[0], num_items=X.shape[1], \n",
    "                    MF_embedding_dim=MF_embedding_dim, \n",
    "                    MLP_embedding_dim=MLP_embedding_dim,\n",
    "                    alpha=alpha,\n",
    "                    regularization_constant=regularization_constant,\n",
    "                )\n",
    "\n",
    "optimizer = torch.optim.SGD(neumf.parameters(), lr=lr)\n",
    "neumf.to(device)\n",
    "neumf.predict_single_interaction(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a656c07-b5bb-4846-88b3-5b6c4c773be3",
   "metadata": {},
   "source": [
    "### Assign the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01b5fbad-5a7f-4189-ad18-7b329cf6b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pretrained(NeuMF_model, GMF_model, MLP_model):\n",
    "    # Latent weights\n",
    "    NeuMF_model.MF_user_factors.weight = GMF_model.user_factors.weight\n",
    "    NeuMF_model.MF_item_factors.weight = GMF_model.item_factors.weight\n",
    "    NeuMF_model.MLP_user_factors.weight = MLP_model.user_factors.weight\n",
    "    NeuMF_model.MLP_item_factors.weight = MLP_model.item_factors.weight\n",
    "    \n",
    "    # MLP weights\n",
    "    NeuMF_model.fc1.weight = MLP_model.fc1.weight\n",
    "    NeuMF_model.fc2.weight = MLP_model.fc2.weight\n",
    "    NeuMF_model.fc3.weight = MLP_model.fc3.weight\n",
    "    \n",
    "init_pretrained(neumf, gmf, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4dc24-e255-4f47-a153-ec01b2856be3",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45594946-1156-49a7-8b16-c44c500b5bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 188.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 MSE Loss: 1.83, valid MSE Loss: 3.264, valid RMSE Loss: 1.807, valid MAE Loss: 1.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 178.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 MSE Loss: 1.659, valid MSE Loss: 3.128, valid RMSE Loss: 1.769, valid MAE Loss: 1.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 182.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 MSE Loss: 1.619, valid MSE Loss: 3.079, valid RMSE Loss: 1.755, valid MAE Loss: 1.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 212.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 MSE Loss: 1.609, valid MSE Loss: 3.06, valid RMSE Loss: 1.749, valid MAE Loss: 1.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 196.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 MSE Loss: 1.606, valid MSE Loss: 3.05, valid RMSE Loss: 1.746, valid MAE Loss: 1.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 196.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 MSE Loss: 1.604, valid MSE Loss: 3.046, valid RMSE Loss: 1.745, valid MAE Loss: 1.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 200.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 MSE Loss: 1.604, valid MSE Loss: 3.043, valid RMSE Loss: 1.744, valid MAE Loss: 1.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 208.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 MSE Loss: 1.603, valid MSE Loss: 3.042, valid RMSE Loss: 1.744, valid MAE Loss: 1.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 216.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 MSE Loss: 1.602, valid MSE Loss: 3.041, valid RMSE Loss: 1.744, valid MAE Loss: 1.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 187.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 MSE Loss: 1.601, valid MSE Loss: 3.04, valid RMSE Loss: 1.744, valid MAE Loss: 1.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 187.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 MSE Loss: 1.6, valid MSE Loss: 3.04, valid RMSE Loss: 1.743, valid MAE Loss: 1.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 201.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 MSE Loss: 1.599, valid MSE Loss: 3.039, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 177.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 MSE Loss: 1.598, valid MSE Loss: 3.039, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 195.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 MSE Loss: 1.598, valid MSE Loss: 3.038, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 184.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 MSE Loss: 1.597, valid MSE Loss: 3.038, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 184.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 MSE Loss: 1.596, valid MSE Loss: 3.038, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 185.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 MSE Loss: 1.595, valid MSE Loss: 3.038, valid RMSE Loss: 1.743, valid MAE Loss: 1.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 195.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 MSE Loss: 1.594, valid MSE Loss: 3.037, valid RMSE Loss: 1.743, valid MAE Loss: 1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 196.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 MSE Loss: 1.594, valid MSE Loss: 3.036, valid RMSE Loss: 1.743, valid MAE Loss: 1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 220.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 MSE Loss: 1.593, valid MSE Loss: 3.037, valid RMSE Loss: 1.743, valid MAE Loss: 1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 216.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 MSE Loss: 1.592, valid MSE Loss: 3.036, valid RMSE Loss: 1.743, valid MAE Loss: 1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 203.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 MSE Loss: 1.592, valid MSE Loss: 3.036, valid RMSE Loss: 1.742, valid MAE Loss: 1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 194.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 MSE Loss: 1.59, valid MSE Loss: 3.035, valid RMSE Loss: 1.742, valid MAE Loss: 1.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 184.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 MSE Loss: 1.591, valid MSE Loss: 3.036, valid RMSE Loss: 1.742, valid MAE Loss: 1.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 508/508 [00:02<00:00, 183.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 MSE Loss: 1.589, valid MSE Loss: 3.035, valid RMSE Loss: 1.742, valid MAE Loss: 1.343\n"
     ]
    }
   ],
   "source": [
    "train_v2(train_X, valid_X, neumf, optimizer, n_epochs=25, batch_size=64, rmse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88634e08-b868-4c34-b0dd-66762819ea5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "273f1066-9813-4096-b953-2155f3b4ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE Loss: 2.931, test RMSE Loss: 1.712, test MAE Loss: 1.33\n"
     ]
    }
   ],
   "source": [
    "total_test_loss = eval_MSE_loss(test_X, neumf)\n",
    "total_test_RMSE_loss = eval_RMSE_loss(test_X, neumf)\n",
    "total_test_MAE_loss = eval_MAE_loss(test_X, neumf)\n",
    "print(f\"test MSE Loss: {total_test_loss}, test RMSE Loss: {total_test_RMSE_loss}, test MAE Loss: {total_test_MAE_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089982d5-b350-4beb-b053-6a5184d2e119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
