{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8d309f-6b73-4697-95f4-eb449c2ede62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d4bcbd-9676-4775-aa2f-990f305c3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter\n",
    "import multiprocessing\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.formula.api as smf\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd0955-a340-4b06-9c5b-d94ad324e4d1",
   "metadata": {},
   "source": [
    "# Loading Processed Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62734bd0-0272-4a10-9622-7a34a46a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROCESSED_FOLDER = './data/processed/'\n",
    "PROCESSED_REVIEWS_FILE = 'processed_reviews_with_sentiment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41db13ef-f58f-4be8-b789-5f5f94e7c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(os.path.join(PROCESSED_FOLDER, PROCESSED_REVIEWS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f769cfe-cd8f-4a96-bc0c-569c5dfabd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neg_sent_avg</th>\n",
       "      <th>pos_sent_avg</th>\n",
       "      <th>compound_sent_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>First things first. My \"reviews\" system is exp...</td>\n",
       "      <td>8</td>\n",
       "      <td>[['First', 'things', 'first', '.'], ['My', '``...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.168937</td>\n",
       "      <td>0.208675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259117</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Let me start off by saying that Made in Abyss ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['Let', 'me', 'start', 'off', 'by', 'saying',...</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.062741</td>\n",
       "      <td>0.177963</td>\n",
       "      <td>0.302804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253664</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Art 9/10: It is great, especially the actions ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[['Art', '9/10', ':', 'It', 'is', 'great', ','...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>0.159833</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247454</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>As someone who loves Studio Ghibli and its mov...</td>\n",
       "      <td>6</td>\n",
       "      <td>[['As', 'someone', 'who', 'loves', 'Studio', '...</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.187896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23791</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>code geass is one of those series that everybo...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['code', 'geass', 'is', 'one', 'of', 'those',...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.248143</td>\n",
       "      <td>0.534129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  user_id  item_id  \\\n",
       "0     255938        0        1   \n",
       "1     259117        1        2   \n",
       "2     253664        2        3   \n",
       "3     247454        3        4   \n",
       "4      23791        4        5   \n",
       "\n",
       "                                                text  rating  \\\n",
       "0  First things first. My \"reviews\" system is exp...       8   \n",
       "1  Let me start off by saying that Made in Abyss ...      10   \n",
       "2  Art 9/10: It is great, especially the actions ...       7   \n",
       "3  As someone who loves Studio Ghibli and its mov...       6   \n",
       "4  code geass is one of those series that everybo...      10   \n",
       "\n",
       "                                      tokenized_text    neg  neg_sent_avg  \\\n",
       "0  [['First', 'things', 'first', '.'], ['My', '``...  0.094      0.073333   \n",
       "1  [['Let', 'me', 'start', 'off', 'by', 'saying',...  0.081      0.062741   \n",
       "2  [['Art', '9/10', ':', 'It', 'is', 'great', ','...  0.051      0.047278   \n",
       "3  [['As', 'someone', 'who', 'loves', 'Studio', '...  0.083      0.055577   \n",
       "4  [['code', 'geass', 'is', 'one', 'of', 'those',...  0.016      0.028857   \n",
       "\n",
       "   pos_sent_avg  compound_sent_avg  \n",
       "0      0.168937           0.208675  \n",
       "1      0.177963           0.302804  \n",
       "2      0.159833           0.220767  \n",
       "3      0.135269           0.187896  \n",
       "4      0.248143           0.534129  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8dc21-b48d-47c4-92b9-5ecade9e196b",
   "metadata": {},
   "source": [
    "# Converting Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13a2335-8f0b-4ad8-9590-912dc3871c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert item_id to 0 indexed\n",
    "if min(reviews['item_id']) != 0:\n",
    "    reviews['item_id'] = reviews['item_id'] - 1\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2066316-854c-4484-88cc-8043d5eb0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Review:\n",
    "    user_id: int\n",
    "    item_id: int\n",
    "    rating: int\n",
    "    text: str\n",
    "    pos_sent: float\n",
    "    neg_sent: float\n",
    "    compound_sent: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7786b8c-8d87-4e40-aea0-1d5a7933e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_KEY = 'user_id'\n",
    "ITEM_KEY = 'item_id'\n",
    "RATING_KEY = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67804fba-a838-4d53-ab38-dfc529312286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neg_sent_avg</th>\n",
       "      <th>pos_sent_avg</th>\n",
       "      <th>compound_sent_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>First things first. My \"reviews\" system is exp...</td>\n",
       "      <td>8</td>\n",
       "      <td>[['First', 'things', 'first', '.'], ['My', '``...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.168937</td>\n",
       "      <td>0.208675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259117</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Let me start off by saying that Made in Abyss ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['Let', 'me', 'start', 'off', 'by', 'saying',...</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.062741</td>\n",
       "      <td>0.177963</td>\n",
       "      <td>0.302804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253664</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Art 9/10: It is great, especially the actions ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[['Art', '9/10', ':', 'It', 'is', 'great', ','...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>0.159833</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247454</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>As someone who loves Studio Ghibli and its mov...</td>\n",
       "      <td>6</td>\n",
       "      <td>[['As', 'someone', 'who', 'loves', 'Studio', '...</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.187896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23791</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>code geass is one of those series that everybo...</td>\n",
       "      <td>10</td>\n",
       "      <td>[['code', 'geass', 'is', 'one', 'of', 'those',...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.248143</td>\n",
       "      <td>0.534129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  user_id  item_id  \\\n",
       "0     255938        0        0   \n",
       "1     259117        1        1   \n",
       "2     253664        2        2   \n",
       "3     247454        3        3   \n",
       "4      23791        4        4   \n",
       "\n",
       "                                                text  rating  \\\n",
       "0  First things first. My \"reviews\" system is exp...       8   \n",
       "1  Let me start off by saying that Made in Abyss ...      10   \n",
       "2  Art 9/10: It is great, especially the actions ...       7   \n",
       "3  As someone who loves Studio Ghibli and its mov...       6   \n",
       "4  code geass is one of those series that everybo...      10   \n",
       "\n",
       "                                      tokenized_text    neg  neg_sent_avg  \\\n",
       "0  [['First', 'things', 'first', '.'], ['My', '``...  0.094      0.073333   \n",
       "1  [['Let', 'me', 'start', 'off', 'by', 'saying',...  0.081      0.062741   \n",
       "2  [['Art', '9/10', ':', 'It', 'is', 'great', ','...  0.051      0.047278   \n",
       "3  [['As', 'someone', 'who', 'loves', 'Studio', '...  0.083      0.055577   \n",
       "4  [['code', 'geass', 'is', 'one', 'of', 'those',...  0.016      0.028857   \n",
       "\n",
       "   pos_sent_avg  compound_sent_avg  \n",
       "0      0.168937           0.208675  \n",
       "1      0.177963           0.302804  \n",
       "2      0.159833           0.220767  \n",
       "3      0.135269           0.187896  \n",
       "4      0.248143           0.534129  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cf6817-3799-4833-85b7-cf46631c16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_reviews = defaultdict(list)\n",
    "for _, row in reviews.iterrows():\n",
    "    user_id, item_id, rating, text = row[USER_KEY], row[ITEM_KEY], row[RATING_KEY], row['text']\n",
    "    pos_sent, neg_sent, compound_sent = row['pos_sent_avg'], row['neg_sent_avg'], row['compound_sent_avg']\n",
    "    user_to_reviews[user_id].append(Review(user_id, item_id, rating, text, pos_sent, neg_sent, compound_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aac691-4002-4fbb-89fb-905f46d5c993",
   "metadata": {},
   "source": [
    "## Creating the score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2741bc-2256-4222-85b3-12148559e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users by items\n",
    "X = np.zeros(shape=(reviews['user_id'].nunique(), reviews['item_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b66889-3914-4661-87c9-4d6bcb04b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in reviews.iterrows():\n",
    "    user_id, item_id, rating = row[USER_KEY], row[ITEM_KEY], row[RATING_KEY]\n",
    "    X[user_id][item_id] = rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4cbb9-e530-4f74-a040-8e25ff0400be",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f8871e-2a53-4338-a7db-42c251f91baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = copy.deepcopy(X)\n",
    "valid_X = np.zeros(shape=X.shape)\n",
    "test_X = np.zeros(shape=X.shape)\n",
    "\n",
    "for user_id, reviews in user_to_reviews.items():\n",
    "    # can confirm this actually shuffles properly (this code block works)\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    # Leave one out for valid\n",
    "    valid_review = reviews[0]\n",
    "    train_X[valid_review.user_id][valid_review.item_id] = 0\n",
    "    valid_X[valid_review.user_id][valid_review.item_id] = valid_review.rating\n",
    "    \n",
    "    # Leave one out for test\n",
    "    test_review = reviews[1]\n",
    "    train_X[test_review.user_id][test_review.item_id] = 0\n",
    "    test_X[test_review.user_id][test_review.item_id] = test_review.rating\n",
    "    \n",
    "    # Rest for train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc5daa-0f72-4c5e-968a-b1adea2a7a3d",
   "metadata": {},
   "source": [
    "## Creating bias terms for users / items from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e4f205-bd38-41b7-8bc0-d315ff486b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users\n",
    "user_to_pos_sent = defaultdict(list)\n",
    "user_to_neg_sent = defaultdict(list)\n",
    "user_to_compound_sent = defaultdict(list)\n",
    "\n",
    "# items\n",
    "item_to_pos_sent = defaultdict(list)\n",
    "item_to_neg_sent = defaultdict(list)\n",
    "item_to_compound_sent = defaultdict(list)\n",
    "\n",
    "# loadding\n",
    "for user_id, reviews in user_to_reviews.items():\n",
    "    for r in reviews:\n",
    "        # skip if not in train\n",
    "        if train_X[user_id, r.item_id] == 0:\n",
    "            continue\n",
    "        user_to_pos_sent[user_id].append(r.pos_sent)\n",
    "        user_to_neg_sent[user_id].append(r.neg_sent)\n",
    "        user_to_compound_sent[user_id].append(r.compound_sent)\n",
    "        item_to_pos_sent[r.item_id].append(r.pos_sent)\n",
    "        item_to_neg_sent[r.item_id].append(r.neg_sent)\n",
    "        item_to_compound_sent[r.item_id].append(r.compound_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d030b3d-53be-433c-a5ff-3e0c774d3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging values to get bias term\n",
    "def list_mapping_to_float_mapping(hm: dict):\n",
    "    id_to_sent_term = defaultdict(float)\n",
    "    for k, v in hm.items():\n",
    "        id_to_sent_term[k] = np.mean(v)\n",
    "    return id_to_sent_term\n",
    "\n",
    "user_to_pos_sent_term = list_mapping_to_float_mapping(user_to_pos_sent)\n",
    "user_to_neg_sent_term = list_mapping_to_float_mapping(user_to_neg_sent)\n",
    "user_to_compound_sent_term = list_mapping_to_float_mapping(user_to_compound_sent)\n",
    "item_to_pos_sent_term = list_mapping_to_float_mapping(item_to_pos_sent)\n",
    "item_to_neg_sent_term = list_mapping_to_float_mapping(item_to_neg_sent)\n",
    "item_to_compound_sent_term = list_mapping_to_float_mapping(item_to_compound_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d1ea0-2053-4428-b586-df8fca47c966",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MF with sentiment as a bias(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b075113-a9f4-4ee3-9d09-743659ddc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(values):\n",
    "    return torch.sum(torch.square(values))\n",
    "\n",
    "class SentimentMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=20, regularization_constant=1e-6, sentiment_regularization_constant=1e-6, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_factors = nn.Embedding(num_items, embedding_dim)\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Our two new embeddings: Let's use the sentiment scores!\n",
    "        self.user_sentiment_emb = nn.Embedding(num_users, 3)\n",
    "        self.item_sentiment_emb = nn.Embedding(num_items, 3)\n",
    "        self.sentiment_regularization_constant = sentiment_regularization_constant\n",
    "        \n",
    "    def forward(self, user: torch.LongTensor, item: torch.LongTensor):\n",
    "        # user is shape (users, 1)\n",
    "        # item is shape (items, 1)\n",
    "        # embedding output shape is (*, emb_dim) = (users/items, emb_dim)\n",
    "        user_latent_factors = self.user_factors(user)\n",
    "        item_latent_factors = self.item_factors(item)\n",
    "        pred_rating = user_latent_factors @ item_latent_factors.T\n",
    "        \n",
    "        \n",
    "        user_ids_list = [u.item() for u in user]\n",
    "        item_ids_list = [i.item() for i in item]\n",
    "        user_sent = torch.FloatTensor([[user_to_pos_sent_term[user_id], user_to_neg_sent_term[user_id], user_to_compound_sent_term[user_id]] for user_id in user_ids_list])\n",
    "        item_sent = torch.FloatTensor([[item_to_pos_sent_term[item_id], item_to_neg_sent_term[item_id], item_to_compound_sent_term[item_id]] for item_id in item_ids_list])\n",
    "        \n",
    "        # (users x 3) @ (users x 3).T = (users x 3) @ (3 x users) => (users x users)\n",
    "        user_sentiment_bias = self.user_sentiment_emb(user) @ user_sent.T\n",
    "        item_sentiment_bias = self.item_sentiment_emb(item) @ item_sent.T\n",
    "        \n",
    "        # Add our sentiment bias\n",
    "        pred_rating += (user_sentiment_bias + item_sentiment_bias)\n",
    "        \n",
    "        # Clip in the desired range\n",
    "        pred_rating = 1 + 9 * torch.sigmoid(pred_rating)        \n",
    "        return pred_rating.diagonal()\n",
    "    \n",
    "    def loss(self, pred_rating: torch.LongTensor, rating: torch.LongTensor, rmse=False):\n",
    "        if rmse:\n",
    "            loss = torch.sqrt(F.mse_loss(pred_rating, rating) + self.eps)\n",
    "        else:\n",
    "            loss = F.mse_loss(pred_rating, rating) + self.eps\n",
    "        \n",
    "        \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_values = l2_regularization(self.user_factors.weight) + l2_regularization(self.item_factors.weight)\n",
    "        l2_penalty = (1/len(rating)) * self.regularization_constant * sum_of_squared_values\n",
    "        \n",
    "        sentiment_l2 = (1/len(rating)) * self.sentiment_regularization_constant * (l2_regularization(self.user_sentiment_emb.weight) + l2_regularization(self.item_sentiment_emb.weight))\n",
    "        l2_penalty += sentiment_l2\n",
    "        \n",
    "        # Total Loss\n",
    "        total_loss = loss + l2_penalty\n",
    "        return total_loss\n",
    "    \n",
    "#     def RMSE_loss(self, pred_rating: torch.LongTensor, rating: torch.LongTensor):\n",
    "#         # RMSE\n",
    "#         RMSE_loss = torch.sqrt(F.mse_loss(pred_rating, rating) + self.eps)\n",
    "        \n",
    "#         # L2 Regularization\n",
    "#         sum_of_squared_values = l2_regularization(self.user_factors.weight) + l2_regularization(self.item_factors.weight)\n",
    "#         sum_of_squared_values += l2_regularization(self.user_sentiment_emb.weight) + l2_regularization(self.item_sentiment_emb.weight)\n",
    "#         l2_penalty = (1/len(rating)) * self.regularization_constant * sum_of_squared_values\n",
    "        \n",
    "#         # Total Loss\n",
    "#         total_loss = RMSE_loss + l2_penalty\n",
    "#         return total_loss\n",
    "    \n",
    "    def predict_single_interaction(self, user_id: int, item_id: int):\n",
    "        user = torch.LongTensor([user_id])\n",
    "        item = torch.LongTensor([item_id])\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3ccfc7c5-88ec-47ad-aced-9f57b6e60bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_MSE_loss(eval_X, model, round_digits=3):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)])\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(F.mse_loss(pred_ratings, gt_ratings).item(), 3)\n",
    "\n",
    "def eval_RMSE_loss(eval_X, model):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)])\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(torch.sqrt(F.mse_loss(pred_ratings, gt_ratings)).item(), 3)\n",
    "\n",
    "def eval_MAE_loss(eval_X, model):\n",
    "    \"\"\"Uses reduction mean\"\"\"\n",
    "    user_ids_list, item_ids_list = eval_X.nonzero()\n",
    "    gt_ratings = torch.FloatTensor([eval_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)])\n",
    "    curr_users_tensor = torch.LongTensor(user_ids_list)\n",
    "    curr_items_tensor = torch.LongTensor(item_ids_list)\n",
    "    pred_ratings = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "    \n",
    "    return round(F.l1_loss(pred_ratings, gt_ratings).item(), 3)\n",
    "\n",
    "\n",
    "def train_v2(train_X, valid_X, model, optimizer, n_epochs=10, batch_size=5, rmse=False):\n",
    "    \"\"\"Training Function, calculates training and validation loss\"\"\"\n",
    "    \n",
    "    for epoch in (range(1, n_epochs+1)):\n",
    "        users, items = train_X.nonzero()\n",
    "        num_examples = len(users)\n",
    "        permuted_indices = np.random.permutation(num_examples)\n",
    "        users, items = users[permuted_indices], items[permuted_indices]\n",
    "        \n",
    "\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(num_examples // batch_size)):\n",
    "            user_ids_list = users[i*batch_size:i*batch_size+batch_size]\n",
    "            item_ids_list = items[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Turn data into tensors\n",
    "            rating = torch.FloatTensor([train_X[user_id, item_id] for user_id, item_id in zip(user_ids_list, item_ids_list)])\n",
    "            curr_users_tensor = torch.LongTensor(user_ids_list)\n",
    "            curr_items_tensor = torch.LongTensor(item_ids_list)\n",
    "\n",
    "            # Predict and calculate loss\n",
    "            pred_rating = model.forward(curr_users_tensor, curr_items_tensor)\n",
    "            assert pred_rating.shape == rating.shape\n",
    "            \n",
    "            ## SELECTING LOSS HERE\n",
    "            # loss = model.loss(pred_rating, rating)\n",
    "            loss = model.loss(pred_rating, rating, rmse=rmse)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # MSE Loss w/o regularization (just for status updates)\n",
    "            total_train_loss += F.mse_loss(pred_rating, rating, reduction='sum')\n",
    "\n",
    "        # Computing validation loss for display\n",
    "        total_valid_loss = eval_MSE_loss(valid_X, model)\n",
    "        total_valid_RMSE_loss = eval_RMSE_loss(valid_X, model)\n",
    "        total_valid_MAE_loss = eval_MAE_loss(valid_X, model)\n",
    "        \n",
    "        print(f\"Epoch {epoch} MSE Loss: {round(total_train_loss.item() / (batch_size * (num_examples//batch_size)), 3)}, valid MSE Loss: {total_valid_loss}, valid RMSE Loss: {total_valid_RMSE_loss}, valid MAE Loss: {total_valid_MAE_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e0d79-d054-4bef-889e-7f88187c0ba2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11c1db45-4126-4bb2-86bb-eb1f3d8102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these parameter settings are pretty good, just adjust LR after you get low enough => MSE of 3.6)\n",
    "# batch_size=64\n",
    "# Adam\n",
    "# weight decay in regularization constant\n",
    "embedding_dim=200\n",
    "lr=1e-2\n",
    "regularization_constant=1e-2\n",
    "sentiment_regularization_constant=0\n",
    "\n",
    "model = SentimentMF(num_users=X.shape[0], num_items=X.shape[1], \n",
    "                    embedding_dim=embedding_dim, \n",
    "                    regularization_constant=regularization_constant,\n",
    "                    sentiment_regularization_constant=sentiment_regularization_constant\n",
    "                   )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cc136e9f-fe46-4cc5-91b0-ca2ff71bfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=200\n",
    "lr=1e-2\n",
    "regularization_constant=1e-2\n",
    "sentiment_regularization_constant=0\n",
    "\n",
    "model = SentimentMF(num_users=X.shape[0], num_items=X.shape[1], \n",
    "                    embedding_dim=embedding_dim, \n",
    "                    regularization_constant=regularization_constant,\n",
    "                    sentiment_regularization_constant=sentiment_regularization_constant\n",
    "                   )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08558ae4-d362-4023-a851-71d2c73b34e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4d682357-3c54-49ca-b088-4a43a72c21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On-the-fly modifications\n",
    "# lr = 1e-3\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45594946-1156-49a7-8b16-c44c500b5bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 179.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 MSE Loss: 21.11, valid MSE Loss: 16.254, valid RMSE Loss: 4.032, valid MAE Loss: 3.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 179.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 MSE Loss: 10.41, valid MSE Loss: 9.952, valid RMSE Loss: 3.155, valid MAE Loss: 2.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 146.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 MSE Loss: 4.725, valid MSE Loss: 6.357, valid RMSE Loss: 2.521, valid MAE Loss: 1.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 170.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 MSE Loss: 2.772, valid MSE Loss: 4.857, valid RMSE Loss: 2.204, valid MAE Loss: 1.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 180.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 MSE Loss: 1.816, valid MSE Loss: 4.055, valid RMSE Loss: 2.014, valid MAE Loss: 1.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 182.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 MSE Loss: 1.161, valid MSE Loss: 3.717, valid RMSE Loss: 1.928, valid MAE Loss: 1.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 188.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 MSE Loss: 0.879, valid MSE Loss: 3.592, valid RMSE Loss: 1.895, valid MAE Loss: 1.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 172.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 MSE Loss: 0.843, valid MSE Loss: 3.405, valid RMSE Loss: 1.845, valid MAE Loss: 1.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 162.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 MSE Loss: 0.864, valid MSE Loss: 3.347, valid RMSE Loss: 1.83, valid MAE Loss: 1.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 187.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 MSE Loss: 0.844, valid MSE Loss: 3.309, valid RMSE Loss: 1.819, valid MAE Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 168.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 MSE Loss: 0.854, valid MSE Loss: 3.27, valid RMSE Loss: 1.808, valid MAE Loss: 1.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 254/254 [00:01<00:00, 162.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 MSE Loss: 0.849, valid MSE Loss: 3.325, valid RMSE Loss: 1.823, valid MAE Loss: 1.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████▎    | 221/254 [00:01<00:00, 131.39it/s]"
     ]
    }
   ],
   "source": [
    "train_v2(train_X, valid_X, model, optimizer, n_epochs=25, batch_size=128, rmse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88634e08-b868-4c34-b0dd-66762819ea5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "273f1066-9813-4096-b953-2155f3b4ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE Loss: 3.143, test RMSE Loss: 1.773, test MAE Loss: 1.348\n"
     ]
    }
   ],
   "source": [
    "total_test_loss = eval_MSE_loss(test_X, model)\n",
    "total_test_RMSE_loss = eval_RMSE_loss(test_X, model)\n",
    "total_test_MAE_loss = eval_MAE_loss(test_X, model)\n",
    "print(f\"test MSE Loss: {total_test_loss}, test RMSE Loss: {total_test_RMSE_loss}, test MAE Loss: {total_test_MAE_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc25009-27a2-4cb1-af75-42b40a5cfc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
